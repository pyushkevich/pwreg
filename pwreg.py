import SimpleITK as sitk
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

def_device = torch.device("cpu")

class Image2D:

    def __init__(self, T=None, ras=None):
        self.T = T
        self.ras = ras

    @staticmethod
    def load(fname, dtype=torch.short, device=def_device):

        img = sitk.ReadImage(fname)

        # Get the tensor representing the image, unsqueeze for Torch to have dims 1x1xWxH
        t = torch.tensor(sitk.GetArrayFromImage(img), dtype=dtype, device=device).squeeze()
        t = torch.unsqueeze(torch.unsqueeze(t, 0), 0)

        # Construct the NIFTI matrix for the image
        idir, ispc, iorg = img.GetDirection(), img.GetSpacing(), img.GetOrigin()
        idir = np.array(idir).reshape(2, 2) if len(idir) == 4 else np.array(idir).reshape(3, 3)[0:2, 0:2]
        ras = np.eye(3)
        ras[0:2, 0:2] = idir @ np.array([[-ispc[0], 0.], [0., -ispc[1]]])
        ras[2, 0:2] = np.array(iorg[0:2])

        return Image2D(t, ras)

    # Function to generate a 3D Gaussian filter
    @staticmethod
    def gaussian_2d(sigma):
        t_range = np.arange(-np.ceil(3.5 * sigma), np.ceil(3.5 * sigma) + 1)
        gaussian_1d = np.exp(- t_range ** 2 / (2 * sigma ** 2)) / (np.sqrt(2 * np.pi) * sigma)
        return np.tensordot(gaussian_1d, gaussian_1d, axes=0)

    @staticmethod
    def gaussian_fft_kernel(I_ref, sigma):
        """
        Generate the FFT of a Gaussian kernel that can be used for FFT-based Gaussian smoothing

        :param I_ref: 4D tensor that will be convolved with the kernel, used to infer the
                      shape, data type and device for the output kernel
        :param sigma: standard deviation of the Gaussian kernel
        :param dtype: data type of the output tensor (default: torch.float32)
        :returns: 5D tensor that can be multiplied by the image RFFT for FFT-based smoothing
        """

        # Generate a Gaussian kernel, convert to PyTorch
        K = torch.tensor(Image2D.gaussian_2d(sigma),
                         dtype=I_ref.dtype, device=I_ref.device).unsqueeze(0).unsqueeze(0)

        # Normalize the kernel
        K = K / K.sum()

        # Pad the kernel so it is centered in an image of size I_ref.shape
        ab_pad = np.array(I_ref.shape[2:4]) - np.array(K.shape[2:4])
        a_pad = np.floor(ab_pad / 2)
        b_pad = ab_pad - a_pad
        pad_4d = tuple(np.flip(np.vstack((a_pad, b_pad)).T.flatten()).astype(int)) + (0, 0, 0, 0)
        K_pad = F.pad(K, pad_4d)

        # Return the real FFT of the kernel
        return torch.fft.rfftn(K_pad)

    @staticmethod
    def pytorch_gaussian_lpf(img, sigma=None, kernel=None):
        """
        Apply Gaussian smoothing to a 3D image represented as a PyTorch tensor

        You can pass in the sigma of the Gaussian or a 5D tensor representing the Gaussian
        kernel (generated using `my_gaussian_fft_kernel`). The latter is faster if you will
        be making repeated calls to this function.

        :param img: Input image, represented as a 5D tensor
        :param sigma: Standard deviation of the Gaussian kernel. If `kernel` parameter
            is not supplied, the kernel will be generated by calling `my_gaussian_fft_kernel`
        :param kernel: Gaussian kernel generated by `my_gaussian_fft_kernel`.
        """

        # Either sigma or kernel must be provided
        assert sigma is not None or kernel is not None
        if kernel is None:
            kernel = Image2D.gaussian_fft_kernel(img, sigma)

        # Create a Gaussian kernel
        img_fft = torch.fft.rfftn(img)
        return torch.fft.fftshift(torch.fft.irfftn(img_fft * kernel))

    def downsample(self, factor, sigma = None):
        if sigma is None:
            sigma = 0.5 / factor

        T = self.T
        if sigma != 0.0:
            T = Image2D.pytorch_gaussian_lpf(T, sigma)

        x = Image2D()
        x.T = F.interpolate(T, scale_factor=factor, align_corners=False, mode='bilinear', recompute_scale_factor=True)
        rassc = np.eye(3)
        rassc[0:2, 0:2] = np.diag(np.flip(np.array(self.T.shape[-2:]) / np.array(x.T.shape[-2:])))
        x.ras = rassc @ self.ras
        x.ras[0:3, 2] = x.ras[2, 0:3] + ((x.ras - self.ras) @ np.array([0.5, 0.5, 0.]).reshape(3, 1)).squeeze()
        return x


class Dataset:

    def __init__(self, img_fix, img_mov, img_mask, dtype=torch.short):

        # Read the intensity images
        self.I_fix = Image2D.load(img_fix, torch.float)
        self.I_mov = Image2D.load(img_mov, torch.float)

        # Read the chunk mask image
        M_chunk = Image2D.load(img_mask, torch.long)
        T = torch.nn.functional.one_hot(M_chunk.T)[:, :, :, :, 1:].transpose(0, 4).squeeze(4).type(torch.float)
        self.M_fix = Image2D(T, M_chunk.ras)


class ChunkNCC(nn.Module):

    def __init__(self, k, radius=2):
        super(ChunkNCC, self).__init__()
        self.k = k
        self.nccsize = 2 * radius + 1
        self.wmean = torch.ones((6,1,self.nccsize,self.nccsize)) / self.nccsize**2
        self.eps = 0.01

    def ncc_maps(self, I, J, M):

        # Stack M, I, J, I^2, IJ, J^2 into channels
        MI, MJ = M * I, M * J
        Z = torch.cat((M, MI, MJ, MI * I, MI * J, MJ * J), axis=1)

        # Perform convolution
        Zw = F.conv2d(Z, self.wmean, groups=6)

        # Compute signed NCC metric
        n = self.nccsize**2
        eps = 0.01
        var_f = n * Zw[:, 3, :, :] - Zw[:, 1, :, :] ** 2 + eps
        var_m = n * Zw[:, 5, :, :] - Zw[:, 2, :, :] ** 2 + eps
        cov_fm = n * Zw[:, 4, :, :] - Zw[:, 1, :, :] * Zw[:, 2, :, :]

        return cov_fm * torch.abs(cov_fm) / (var_f * var_m)

    def forward(self, I, J, M):
        ncc = self.ncc_maps(I, J, M)
        return ncc.view(self.k,-1).sum(1) / M.view(self.k,-1).sum(1)


class OverlapLoss(nn.Module):

    def __init__(self, ref_shape, src_chunk_mask):
        super(OverlapLoss, self).__init__()
        self.ref_shape = ref_shape
        self.src_chunk_mask = src_chunk_mask
        self.N = torch.sum(src_chunk_mask)

    def overlap_image(self, A):
        grid = F.affine_grid(A[:, 0:2, :], self.ref_shape, align_corners=False)
        mask_to_ref = F.grid_sample(self.src_chunk_mask, grid, align_corners=False)
        return mask_to_ref

    def forward(self, A):
        ovl_counts = torch.sum(self.overlap_image(A), axis=0)
        ovl_loss = 0.5 * (ovl_counts * (ovl_counts - 1.))
        return torch.sum(ovl_loss) / self.N


class RigidProblem(nn.Module):

    def __init__(self, I_fix, I_mov, M_fix, ncc_radius = 2):

        # Standard initialization
        super(RigidProblem, self).__init__()

        # Assign the data
        self.I_fix, self.I_mov, self.M_fix = I_fix, I_mov, M_fix

        # Fix the number of masks, i.e., batch size
        self.k = M_fix.T.shape[0]

        # Generate affine transformation from pytorch grid into RAS coords for fixed image
        h, w = I_fix.T.shape[-2:]
        # U = np.array([[0, w / 2., w / 2. - .5], [h / 2., 0, h / 2. - .5], [0., 0., 1.]])
        U = np.array([[0, h / 2., h / 2. - .5], [w / 2., 0, w / 2. - .5], [0., 0., 1.]])
        self.R_fix = torch.tensor(I_fix.ras @ U).type(torch.float)
        self.Q_fix = torch.tensor(np.linalg.inv(self.R_fix.numpy()))

        # Generate same for the moving image
        h, w = I_mov.T.shape[-2:]
        # U = np.array([[0, w / 2., w / 2. - .5], [h / 2., 0, h / 2. - .5], [0., 0., 1.]])
        U = np.array([[0, h / 2., h / 2. - .5], [w / 2., 0, w / 2. - .5], [0., 0., 1.]])
        self.R_mov = torch.tensor(I_mov.ras @ U).type(torch.float)
        self.Q_mov = torch.tensor(np.linalg.inv(self.R_mov.numpy()))

        # Create the NCC metric
        self.ncc = ChunkNCC(self.k, ncc_radius)

        # Create the overlap metric
        self.ovl = OverlapLoss((self.k,) + I_mov.T.shape[-3:], self.M_fix.T)

    def compute_tform(self, theta, dx, dy):

        # Generate the rigid matrices from the inputs
        M = torch.stack((
                torch.stack((torch.cos(theta), -torch.sin(theta), dx)),
                torch.stack((torch.sin(theta), torch.cos(theta), dy)),
                torch.stack((torch.zeros(self.k), torch.zeros(self.k), torch.ones(self.k)))
            )).transpose(0, 2).transpose(1, 2)

        return self.Q_mov @ M @ self.R_fix

    def apply_tform_to_moving(self, A):

        # Generate a sampling grid for this matrix
        grid = F.affine_grid(A[:, 0:2, :], self.M_fix.T.shape, align_corners=False)

        # Apply sampling grid to moving image
        return F.grid_sample(self.I_mov.T.repeat(self.k,1,1,1), grid, align_corners=False)

    def forward(self, theta, dx, dy):

        # Get the transform and resample moving image
        A = self.compute_tform(theta, dx, dy)
        T_res = self.apply_tform_to_moving(A)

        # Compute NCC loss
        ncc_val = self.ncc(self.I_fix.T, T_res, self.M_fix.T)

        # Compute Overlap loss
        ovl_val = self.ovl(torch.inverse(A))

        return ncc_val, ovl_val


class RigidProblemOptimizer:

    def __init__(self, problem, lam=10.):
        self.p = problem
        self.lam = lam

    @staticmethod
    def make_init_param(k, theta_scalar=0.0, dx_scalar=0.0, dy_scalar=0.0):
        theta = torch.tensor(np.zeros(k) + theta_scalar, requires_grad=True, dtype=torch.float)
        dx = torch.tensor(np.zeros(k) + dx_scalar, requires_grad=True, dtype=torch.float)
        dy = torch.tensor(np.zeros(k) + dy_scalar, requires_grad=True, dtype=torch.float)
        return theta, dx, dy

    def objective(self, theta, dx, dy):
        ncc, ovl = self.p(theta, dx, dy)
        obj = torch.sum(1 - ncc) + self.lam * ovl
        return ncc, ovl, obj

    def optimize(self, theta, dx, dy, n_iter=100):

        optimizer = torch.optim.LBFGS([theta, dx, dy],
                                      history_size=10,
                                      max_iter=4,
                                      line_search_fn="strong_wolfe")

        # Keep track of the objective function values over the course of optimization
        opt_history = []

        # Run for a few iterations
        for i in range(n_iter):
            optimizer.zero_grad()
            ncc, ovl, objective = self.objective(theta, dx, dy)
            objective.backward()
            optimizer.step(lambda: self.objective(theta, dx, dy)[2])
            opt_history.append(objective.item())
            print('Iter %03d NCC: %8.4f, OVL: %8.4f, OBJ: %8.4f, ' % (i, torch.mean(ncc), ovl, objective.item()))

        return opt_history, ncc, ovl









